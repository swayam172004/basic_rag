{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce164a09-52ab-47e4-afce-319339ea67eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` and `dataset.get_index('embeddings').save(index_path)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#HuggingFace RagRetriever expects specific index format; here we use a simple retriever wrapper:\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m retriever \u001b[38;5;241m=\u001b[39m RagRetriever\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m passages\u001b[38;5;241m=\u001b[39mdocs,                \u001b[38;5;66;03m# small example: pass docs directly\u001b[39;00m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m RagSequenceForGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#4) Ask a question\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:451\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[1;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m     index \u001b[38;5;241m=\u001b[39m CustomHFIndex(config\u001b[38;5;241m.\u001b[39mretrieval_vector_size, indexed_dataset)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index(config)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    453\u001b[0m     config,\n\u001b[0;32m    454\u001b[0m     question_encoder_tokenizer\u001b[38;5;241m=\u001b[39mquestion_encoder_tokenizer,\n\u001b[0;32m    455\u001b[0m     generator_tokenizer\u001b[38;5;241m=\u001b[39mgenerator_tokenizer,\n\u001b[0;32m    456\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    457\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:424\u001b[0m, in \u001b[0;36mRagRetriever._build_index\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LegacyIndex(\n\u001b[0;32m    420\u001b[0m         config\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    421\u001b[0m         config\u001b[38;5;241m.\u001b[39mindex_path \u001b[38;5;129;01mor\u001b[39;00m LEGACY_INDEX_PATH,\n\u001b[0;32m    422\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mindex_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CustomHFIndex\u001b[38;5;241m.\u001b[39mload_from_disk(\n\u001b[0;32m    425\u001b[0m         vector_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    426\u001b[0m         dataset_path\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpassages_path,\n\u001b[0;32m    427\u001b[0m         index_path\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_path,\n\u001b[0;32m    428\u001b[0m     )\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CanonicalHFIndex(\n\u001b[0;32m    431\u001b[0m         vector_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    432\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m         dataset_revision\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset_revision,\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:330\u001b[0m, in \u001b[0;36mCustomHFIndex.load_from_disk\u001b[1;34m(cls, vector_size, dataset_path, index_path)\u001b[0m\n\u001b[0;32m    328\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading passages from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m index_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `dataset.get_index(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).save(index_path)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m     )\n\u001b[0;32m    334\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_from_disk(dataset_path)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(vector_size\u001b[38;5;241m=\u001b[39mvector_size, dataset\u001b[38;5;241m=\u001b[39mdataset, index_path\u001b[38;5;241m=\u001b[39mindex_path)\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` and `dataset.get_index('embeddings').save(index_path)`."
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "from datasets import Dataset\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "#1) Prepare docs (list of strings)\n",
    "\n",
    "docs = [\n",
    "\"Earth is the third planet from the Sun.\",\n",
    "\"Jupiter has a strong magnetic field and many moons.\",\n",
    "# ... your doc passages ...\n",
    "]\n",
    "\n",
    "#2) Build simple FAISS index with embeddings (example using sentence-transformers)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "doc_embeddings = embedder.encode(docs, convert_to_numpy=True)\n",
    "\n",
    "d = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "faiss.normalize_L2(doc_embeddings)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "#3) Build HF components\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n",
    "#HuggingFace RagRetriever expects specific index format; here we use a simple retriever wrapper:\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "\"facebook/rag-token-base\",\n",
    "index_name=\"custom\",\n",
    "passages=docs,                # small example: pass docs directly\n",
    ")\n",
    "\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "\n",
    "#4) Ask a question\n",
    "\n",
    "question = \"Which planet is third from the Sun?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "generated = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], do_deduplication=True, num_return_sequences=1)\n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5099f51-4452-465d-8aff-54efde852a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
